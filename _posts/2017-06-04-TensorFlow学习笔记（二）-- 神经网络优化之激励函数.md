---
title: TensorFlow学习笔记（二）-- 神经网络优化之激励函数
date: 2017-06-04 22:46:52
---
#### 激励函数（Activation function）
维基百科的解释是，一个节点的激活函数定义了该节点在给定的输入或输入的集合下的输出。不要误解是指这个函数去激活什么，而是指如何把“激活的神经元的特征”通过函数把特征保留并映射出来，这是神经网络能解决非线性问题的关键。

因为神经网络的数学基础是处处可微的，所以选取的激活函数也要保证数据的输入和输出也是可微的，常见的激活函数有：

| 类型 | 取值范围 | 描述 |
| :--: | :--: | :--: |
| tanh | [-1, 1] | 双切正切函数 |
| sigmoid | [0, 1] | 采用S形函数 |
| ReLu | 大于0的留下，否则一律删除 | 简单而粗暴 |

神经网络中，运算特征是在不断进行循环计算，所以在每代的循环过程中，每个神经元的值也是在不断变化的。这就导致tanh特征相差明显时结果会更理想，在循环过程中会不断扩大特征效果显示出来，但有时候特征相差比较复杂或是相差不大时，就需要更加细粒度的分类去判断，这时sigmoid的效果就会好很多。而ReLu就是取max(0, x)，因为神经网络不断反复计算，实际上就变成了它在不断试探如何用一个大多数为0的矩阵刻画数据特征，因为稀疏特性（数据有冗余，但近似程度的最大保留数据特征可以用大多数元素为0的稀疏举证来实现）的存在，反而这种方法运算快效果又好，所以目前大多用ReLu代替sigmoid函数。

#### 应用
有这样一个需求，将下面的红蓝点进行正确的分类，其实就是二分类问题。

![二分图](http://i2.muimg.com/595056/6c5bc6ddfcab0fb3.jpg)

这时可以通过单层感知机算法找到一条合适的线性方程很容易的将它们划分开：

![单层感知机](http://i2.muimg.com/595056/929dab4a213ee96a.png)

那如果给出的是这样的样本点呢：

![非线性可分](http://i2.muimg.com/595056/b70d38949f806094.png)

可以看到该样本点不是线性可分的，一条直线怎么动都不可能完全正确的将三角形和圆形分开，有同学会说用多个感知器来进行组合，以求获得更大的分类问题，上图我们看下是否可行：

![多层感知机](http://i2.muimg.com/595056/ed74bdd42f1989bf.png)

好的，我们已经得到多层感知机分类器了，那么分类能力是否就能将样本点正确分开呢，还是分析一下，我们得到的$y=w_{2-1}(w_{1-11}x_1+w_{1-21}x_2+b_{1-1})+w_{2-2}(w_{1-12}x_1+w_{1-22}x_2+b_{1-2})+w_{2-3}(w_{1-13}x_1+w_{1-23}x_2+b_{1-3})$，对这个公式合并同类项后得到：$y=x_1(w_{2-1}w_{1-11}+w_{2-2}w_{1-12}+w_{2-3}w_{1-13})+x_2(w_{2-1}w_{1-21}+w_{2-2}w_{1-22}+w_{2-3}w_{1-23})+w_{2-1}b_{1-1}+w_{2-2}b_{1-2}+w_{2-3}b_{1-3}$，大家可以看出，不管它怎么组合，最终都是线性方程的组合，得到的分类器本质上还是一个线性方程，也就仍不能解决非线性问题。

就像下图，直线无论在平面上如何旋转，都不能将样本点分开

![直线旋转图](http://i2.muimg.com/595056/cf5196e334dbb297.png)

既然是非线性问题，总有线性方程不能正确分类的地方，上面的线性方程组合过程其实类似在做如下三条直线的组合，如图：

![三条直线组合](http://i2.muimg.com/595056/27f2d5de68f6b618.png)

但如果在每一层叠加完后，加入一个激活函数，如图所示：

![激活函数](http://i2.muimg.com/595056/25e2ddde47f58fe1.png)

通过这个激活函数映射之后，输出很明显就是一个非线性函数，同理扩展到多个神经元组合，表达能力就会更强。

![多层激活函数](http://i2.muimg.com/595056/fa2ad3f936a1666c.png)

和上面线性组合相对应的非线性组合如下：

![非线性组合](http://i2.muimg.com/595056/c294355fe819f019.png)

最后再通过最优化损失函数，不断训练优化，我们可以学习到能够正确分类的曲线，至于具体是什么样，谁也不知道。。maybe是下面的这个

![非线性曲线](http://i2.muimg.com/595056/99159ff7f409599e.png)

end, 基于以上我们也就解释了一个观点--激活函数是用来加入非线性因素的，解决线性模型不能解决的问题。


***
#### more information
[Performance Analysis of Various Activation Functions in
Generalized MLP Architectures of Neural Networks](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.740.9413&rep=rep1&type=pdf)