---
title: TensorFlow学习笔记（六）-- 卷积神经网络（CNN）反向传播算法
date: 2017-07-31 22:35:52
math: y
---
#### 回顾DNN反向传播算法
在前面已经对DNN反向传播算法做了介绍，这里只给出简单推导过程

![DNN反向传播1](/images/figures/2017-07-31-01.jpg)

![DNN反向传播2](/images/figures/2017-07-31-02.jpg)

有了$W$,$b$梯度表达式，就可以用梯度下降法来优化$W$,$b$，求出最终的所有$W$,$b$的值。

现在我们想把同样的思想用到CNN中，很明显，CNN有些不同的地方，不能直接去套用DNN的反向传播算法的公式。

#### CNN反向传播算法思想
要想套用DNN的反向传播算法到CNN，要解决以下问题：

- 池化层在前向传播的时候，对输入进行了压缩，那么我们现在需要向前反向推导$\delta^{l-1}$，这个推导方法和DNN完全不同。
- 卷积层是通过张量卷积，或者说若干个矩阵卷积求和而得的当前层的输出，这和DNN很不相同，DNN的全连接层是直接进行矩阵乘法得到当前层的输出。这样在卷积层反向传播的时候，上一层的$\delta^{l-1}$递推计算方法肯定有所不同。
- 对于卷积层，由于W使用的运算是卷积，那么从$\delta^l$推导出该层的所有卷积核的$W$,$b$的方式也不同。

#### CNN反向传播过程
推导过程也就是依次解决以上问题的过程，需要注意的是，由于卷积层可以有多个卷积核，各个卷积核的处理方法是完全相同且独立的，为了简化算法公式的复杂度，我们下面提到卷积核都是卷积层中若干卷积核中的一个。

![CNN反向传播](/images/figures/2017-07-31-03.jpg)

可以看出最后的结果和DNN类似，不同点在于对含有卷积核的式子求导时，卷积核被旋转了180度，即式子中的$rot180()$，翻转180度的意思是上下翻转一次，接着左右翻转一次，而在DNN中这里只是矩阵的转置，那么为什么呢，接下来就以一个简单的例子来说明。

![卷积核翻转1](/images/figures/2017-07-31-04.jpg)

![卷积核翻转2](/images/figures/2017-07-31-05.jpg)

最后的矩阵表示，为了符合梯度计算，就在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差，也就直观说明了在对含有卷积的式子求导时，卷积核需要翻转180度。