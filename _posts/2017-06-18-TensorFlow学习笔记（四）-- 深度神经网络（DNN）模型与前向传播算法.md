---
title: TensorFlow学习笔记（四）-- 深度神经网络（DNN）模型与前向传播算法
date: 2017-06-18 22:45:52
math: y
---
深度神经网络（Deep Neural Networks，简称DNN）是深度学习的基础，而要理解DNN，首先我们要搞清楚DNN模型，以下就是对DNN模型和前向传播算法的理解与总结。

#### 从感知机到神经网络
感知机模型比较简单，它是一个有若干输入和一个输出的模型，如下图：

![感知机模型](/images/figures/2017-06-18-01.png)

输出和输入之间学习到一个线性关系，从而得到中间输出结果：

$$z=\sum_{i=1}^mw_ix_i+b$$

紧接着是一个神经元激活函数：

$$sign(z)=\begin{cases}
          -1 & z<0 \\
           1 & z>=0
           \end{cases}$$

从而得到我们想要的输出结果。很容易看出这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。

而神经网络则在感知机模型上做了扩展，主要有以下三点：

1）加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图所示，当然增加了这么多隐藏层模型的复杂度也增加了好多。

![神经网络一](/images/figures/2017-06-18-02.png)

2）输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。

![神经网络二](/images/figures/2017-06-18-03.png)

3）对激活函数做扩展，感知机的激活函数是 $sign(z)$ ，虽然简单但是处理能力有限，因此神经网络中一般使用其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：

$$f(z)=\dfrac{1}{1+e^{-z}}$$

或者是之前有说过的tanx，softmax和ReLu等，通过使用不同的激活函数，神经网络的表达能力得到了进一步增强。

#### DNN的基本结构
DNN可以理解为有很多隐藏层的神经网络，这个很多其实也没有什么度量标准，多层神经网络和深度神经网络DNN其实也是指的一个东西，当然，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）, 名字实在是多。后面我们讲到的神经网络都默认为DNN。

将DNN内部的神经网络按不同层的位置划分可以分为三类，输入层，隐藏层和输出层，如下图所示:

![DNN基本结构](/images/figures/2017-06-18-04.png)

**层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。** 虽然DNN看起来很发杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系 $z=\sum w_ix_i+b$ 加上一个激活函数 $\kappa(z)$ 。

由于DNN层数很多，则我们的线性关系系数 $w$ 和偏置 $b$ 的数量也就很多了，具体的参数在DNN中是如何定义的尼。

首先我们来看看线性关系系数 $w$ 的定义。以下图一个三层的DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性系数定义为 $w_{24}^3$ 。上标3代表线性系数 $w$ 所在的层数，而下标对应的是输出的第三层索引2和输入的第二层索引4。你也许会问，为什么不是 $w_{42}^3$ , 而是 $w_{24}^3$ 尼？这主要是为了便于模型用于矩阵表示运算，如果是 $w_{42}^3$ 而每次进行矩阵运算是 $w^Tx+b$ ，需要进行转置。将输出的索引放在前面的话，则线性运算不用转置,即直接为 $w^Tx+b$ 。总结下，第 $l-1$ 层的第k个神经元到第 $l$ 层的第j个神经元的线性系数定义为 $w_{jk}^l$ 。注意，输入层是没有 $w$ 参数的。

![w的定义](/images/figures/2017-06-18-05.png)

再来看看偏倚 $b$ 的定义。还是以这个三层的DNN为例，第二层的第三个神经元对应的偏倚定义为 $b_3^2$ 。其中，上标2代表所在的层数，下标3代表偏倚所在的神经元的索引。同样的道理，第三个的第一个神经元的偏倚应该表示为 $b_1^3$ 。同样的，输入层是没有偏倚参数 $b$ 的。

![b的定义](/images/figures/2017-06-18-06.png)

#### DNN前向传播算法数学原理
我们已经知道了DNN各层线性关系系数 $w$ ，偏置 $b$ 的定义，再假设我们选择的激活函数是 $\kappa(z)$ ，隐藏层和输出层的输出值为 $a$ ，则对于下图的三层DNN，利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。

![数学原理](/images/figures/2017-06-18-07.png)

我们可以推导出对于第二层的输出：

$$a_1^2=\kappa(z_1^2)=\kappa(w_{11}^2x_1+w_{12}^2x_2+w_{13}^2x_3+b_1^2)$$

$$a_2^2=\kappa(z_2^2)=\kappa(w_{21}^2x_1+w_{22}^2x_2+w_{23}^2x_3+b_2^2)$$

$$a_3^2=\kappa(z_3^2)=\kappa(w_{31}^2x_1+w_{32}^2x_2+w_{33}^2x_3+b_3^2)$$

由第二层的输出可以求出第三层的输出：

$$a_1^3=\kappa(z_1^3)=\kappa(w_{11}^3a_1^2+w_{12}^3a_2^2+w_{13}^3a_3^2+b_3^3)$$ 

将上面的例子一般化，假设第 $l-1$ 层有m个神经元，则对于第 $l$ 层的第j个神经元的输出 $a_j^l$ ，我们有：

$$a_j^l=\kappa(z_j^l)=\kappa(\sum_{k=1}^mw_{jk}^la_k^{l-1}+b_j^l)$$ 

其中如果 $l=2$ ，则对于 $a_k^l$ 即为输入层的 $x_k$ 。

从上面可以看出，使用代数法一个个的表述输出比较复杂，而如果使用矩阵法则比较简洁。假设第 $l-1$ 层共有m个神经元，而第 $l$ 层有n个神经元，则第 $l$ 层的线性系数 $w$ 组成了一个n x m的矩阵 $W^l$ ，其中偏置 $b$ 组成了一个n x 1的向量 $b^l$ ，输出层 $a$ 组成了一个n X 1的向量 $a^l$ ，而第 $l-1$ 层的输出 $a$ 组成了一个m x 1的向量 $a^{l-1}$ ，则用矩阵法表示，第 $l$ 层的输出为：

$$a^l=\kappa(z^l)=\kappa(W^la^{l-1}+b^l)$$ 

这个表示方法简洁漂亮，后面我们的讨论都会基于上面的这个矩阵法表示来。

#### DNN前向传播算法
上面已经对前向传播做了数学推导，其实归纳起来就是利用我们的若干个权重系数矩阵 $W$ ，偏置向量 $b$ 和输入值向量x进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直运算到输出层，得到输出结果。

输入：总层数L，所有隐藏层和输入层对应的矩阵 $W$ ，偏置向量 $b$ ，输入值向量 $x$ 

输出：输出层的输出 $a^L$ 

1) 初始化 $a^1=x$ 

2) for  $l=2$  to  $L$ ，计算： 

$$a^l=\kappa(z^l)=\kappa(W^la^{l-1}+b^l)$$ 

最后的结果就是输出 $a^L$ 

可能有人会有这样的困惑：单独看DNN前向传播算法，似乎没有什么大用处，而且这一大堆的矩阵 $W$ ，偏倚向量 $b$ 对应的参数怎么获得呢？怎么得到最优的矩阵和偏倚向量尼？这个我们在讲DNN的反向传播算法时再说。
