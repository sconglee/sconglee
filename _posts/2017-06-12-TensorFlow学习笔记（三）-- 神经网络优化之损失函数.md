---
title: TensorFlow学习笔记（三）-- 神经网络优化之损失函数
date: 2017-06-12 20:50:52
math: y
---
#### 损失函数（loss function）
用于决定训练过程如何来“惩罚”网络的预测结果和真实结果的差异，它通常存在于网络的最后一层。本次主要介绍一些神经网络中常用的损失函数以及如何选择合适的损失函数。

损失函数，顾名思义就是模型对数据拟合程度的反映，拟合的越差，损失函数的值就越大。但同时我们也期望，损失函数在比较大时，对应的梯度也要越大，这样的话更新变量的速度就可以加快。我们应该都接触过的“距离”这一概念也可以被用在损失函数里，其对应的就是最小平方误差（MSE）:

 $minC(Y,G(X))=||G(x)-Y||^2=\sum_{i=1}^n(G(X_i)-y_i)^2$ 

其中G是我们的模型，它根据输入矩阵x输出一个预测向量G(x)，这个损失函数的直观意义相当明确：预测值G(x)和真实值Y的距离越大、损失也就越大，反而就越小。它的求导过程也是相当平凡的：

 $\frac{\partial C}{\partial w}=2\sum_{i=1}^n(G(x_i)-y_i)\frac{\partial G(x_i)}{\partial w}$ 

其中w是模型G中的一个待训练的参数，由于MSE比较简单、所以我们能够从一般意义上来讨论它，事实上损失函数的选择通常都是结合激活函数的，下面将讨论几种使用较多的组合方式。

#### 均方误差损失函数+Sigmoid激活函数
Sigmoid激活函数的表达式为：

 $\kappa(z)=\frac{1}{1+e^{-z}}$ 

 $\kappa(z)$ 的函数图像如下：

![Sigmoid](http://i4.piimg.com/595056/247138c0f50336a0.jpg)

从图上可以看出，对于Sigmoid，当z的取值越来越大后，函数曲线变得越来越平缓，意味着此时的导数 $\kappa'(z)$ 也越来越小，同样当z的取值越来越来小时，也有这个问题，仅仅在z取值为0附近时，导数 $\kappa'(z)$ 的取值较大。

而在均方误差+Sigmoid的反向传播算法中，每一层向前递推都要乘以 $\kappa'(z)$ ，得到梯度变化值，这意味着大多时候我们的得到的梯度变化值都很小，导致我们的W,b更新到极值的速度较慢，也就是我们的算法收敛速度较慢、不能接受。
#### 交叉熵损失函数+Sigmoid激活函数
Sigmoid函数特性导致了反向传播算法收敛速度较慢，那么如何改进呢？一种常见的选择是用交叉熵损失函数来代替均方误差损失函数。

每个样本的交叉熵损失函数形式：

 $J(W,b,a,y)=-y.lna-(1-y).ln(1-a)$ 

其中.为向量内积。

那么使用了交叉熵函数就能解决Sigmoid收敛速度慢的问题么，我们看下使用交叉熵时，各层 $\phi^l $ 的梯度情况：
 $\phi^l=\frac{\partial J(W,b,a^l,y)}{\partial z^l}
=-y\frac{1}{a^l}(a^l)(1-a^l)+(1-y)\frac{1}{1-a^l}(a^l)(1-a^l)
=-y(1-a^l)+(1-y)a^l
=a^l-y
$ 

可见此时 $\phi^l $ 梯度表达式中已经没有 $\kappa'(z)$ ，而均方误差损失函数的梯度是：

 $\frac{\partial J(W,b,x,y)}{\partial z^L}=(a^L-y).\kappa'(z)$ 

对比两者在L层的梯度表达式，就可以看出，使用交叉熵得到梯度表达式没有了 $\kappa'(z)$ ，梯度为预测值和真实值得差距，因此避免了反向传播收敛速度慢的问题。

通常情况下，如果我们使用了sigmoid激活函数，交叉熵损失函数肯定比均方差损失函数好用。

#### 对数似然损失函数+softmax激活函数
在前面所说的DNN相关知识中，我们都是假设输出是连续可导的，但如果是分类问题，那么输出应该是一个个的类别，我们该如何利用DNN类解决这个问题尼？

比如假设我们有一个三个类别的分类问题，这样我们的DNN输出层应该有三个神经元，假设第一个神经元对应类别一，第二个对应类别二，第三个对应类别三，这样我们期望的输出应该是(1,0,0)，（0,1,0）和(0,0,1)这三种。即样本真实类别对应的神经元输出应该无限接近或者等于1，而非改样本真实输出对应的神经元的输出应该无限接近或者等于0。或者说，我们希望输出层的神经元对应的输出是若干个概率值，这若干个概率值即我们DNN模型对于输入值对于各类别的输出预测，同时为满足概率模型，这若干个概率值之和应该等于1。

DNN分类模型要求是输出层神经元输出的值在0到1之间，同时所有输出值之和为1。很明显，现有的普通DNN是无法满足这个要求的。但是我们只需要对现有的全连接DNN稍作改良，即可用于解决分类问题。在现有的DNN模型中，我们可以将输出层第i个神经元的激活函数定义为如下形式：

 $a_i^L=\frac{e^{z_i^L}}{\sum_{j=1}^{n_L}e^{z_j^L}}$ 

其中， $n_L$ 是输出层第L层的神经元个数，也即我们分类问题的类别数。很容易看出，所有的 $a_i^L$ 都是在（0,1）之间的数字，而 $\sum_{j=1}^{n_L}e^{z_j^L}$ 作为归一化因子保证了所有的 $a_i^L$ 之和为1。

可以看出仅仅将输出层的激活函数从Sigmoid之类的函数转变为上面的激活函数即可。它就是我们的softmax激活函数，在分类问题中有广泛的应用。

下面通过一个例子介绍softmax激活函数在前向传播算法时的使用。假设输出层有三个神经元，而未激活的输出为3,1和-3，求出各自的指数表达式20,2.7和0.05，我们的归一化因子是22.75，这样我们就求出了三个类别的概率输出分布为0.88,0.12和0。

![](http://i2.muimg.com/595056/dd085910b46eb689.jpg)

从上图的计算过程可以看出，将softmax用于前向传播算法是很简单的，但在反向传播算法中还简单么，也就是梯度好计算么，答案是肯定的。

对于用于分类的softmax激活函数，对应的损失函数一般都是用对数似然函数：

 $J(W,b,a^L,y)=-\sum_ky_klna_k^L$ 

其中 $y_k$ 的取值为0或者1,如果某一训练样本的输出为第i类，则 $y_i=1$ ，其余都为0。由于每个样本只属于一个类别，所以这个对数似然函数可以简化为：

 $J(W,b,a^L,y)=-lna_i^L$ 

其中i即为训练样本真实的类别序号。

可见损失函数只和真实类别对应的输出有关，这样假设真实类别是第i类，则其他不属于第i类序号对应的神经元的梯度导数直接为0。对于真实类别第i类，它的 $W_i^L$ 对应的梯度计算为：

$$\frac{\partial J(W,b,a^L,y)}{\partial W_i^L}=\frac{\partial J(W,b,a^L,y)}{\partial a_i^L}\frac{\partial a_i^L}{\partial z_i^L}\frac{\partial z_i^L}{\partial w_i^L}

=(a_i^L-1)a_i^{L-1}$$

同样的可以得到 $b_i^L$ 的梯度表达式为：

 $\frac{\partial J(W,b,a^L,y)}{\partial b_i^L}=a_i^L-1$ 

可见，梯度计算也很简洁，也没有第一节说的训练速度慢的问题。举个例子，假如我们对于第2类的训练样本，通过前向算法计算的未激活输出为（1,5,3），则我们得到softmax激活后的概率输出为：(0.015,0.866,0.117)。由于我们的类别是第二类，则反向传播的梯度应该为：(0.015,0.866-1,0.117)。是不是很简单尼？

当softmax输出层的反向传播计算完以后，后面的普通DNN层的反向传播计算和之前讲的普通DNN没有区别。

#### 梯度爆炸梯度消失和ReLu激活函数
学习DNN，大家一定听说过梯度爆炸和梯度消失两个词。尤其是梯度消失，是限制DNN与深度学习的一个关键障碍，目前也还没有完全攻克。

什么是梯度爆炸和梯度消失呢？从理论上说都可以写一篇论文出来。不过简单理解，就是在反向传播的算法过程中，由于我们使用的是矩阵求导的链式法则，有一大串连乘，如果连乘的数字在每层都是小于1的，则梯度越往前乘越小，导致梯度几乎消失，进而导致前面的隐藏层的W,b参数随着迭代的进行，几乎没有大的改变，更谈不上收敛了，这个问题目前仍没有完美的解决办法；而如果连乘的数字在每层都是大于1的，则梯度越往前乘越大，导致梯度爆炸，对于这种情况，我们一般可以调整DNN模型中的初始化参数得以解决。

对于无法完美解决的梯度消失问题，目前也有很多研究，一个可能部分解决梯度消失问题的办法是使用ReLu(Rectified Linear Unit)激活函数，ReLu在卷积神经网络中得到了广泛应用，似乎梯度消失不再是问题，其表达式就是我们之前提到的：

 $\phi(z)=max(0,z)$ 

也就是说大于等于0则不变，小于0则激活后为0。就这么一撮就可以解决梯度消失？至少部分是的。具体原因现在其实也没有从理论上得以证明。

#### Summary
1）如果使用sigmoid激活函数，则交叉熵损失函数一般肯定比均方差损失函数好

2）如果是DNN用于分类，则一般在输出层使用softmax激活函数和对数似然损失函数

3）ReLU激活函数对梯度消失问题有一定程度的解决，尤其是在CNN模型中
